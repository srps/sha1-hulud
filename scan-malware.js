#!/usr/bin/env node

// scan-malware.js - Node.js version of sha1-hulud scanner

const { execSync } = require("child_process");
const fs = require("fs");
const path = require("path");
const os = require("os");
const http = require("http");
const https = require("https");
const { promisify } = require("util");
const readdir = promisify(fs.readdir);
const readFile = promisify(fs.readFile);
const writeFile = promisify(fs.writeFile);

// Parse command-line arguments
function parseArgs() {
  const args = process.argv.slice(2);
  const flags = {
    csv: null,
    root: null,
    download: false,
  };

  for (let i = 0; i < args.length; i++) {
    const arg = args[i];
    if (arg === "-csv" || arg === "--csv") {
      flags.csv = args[++i];
    } else if (arg === "-root" || arg === "--root") {
      flags.root = args[++i];
    } else if (arg === "-download" || arg === "--download") {
      flags.download = true;
    } else if (arg === "-h" || arg === "--help") {
      console.log("Usage: node scan-malware.js [options]");
      console.log("Options:");
      console.log("  -csv <path>     Path to CSV file with package names and versions");
      console.log("  -root <path>    File system root to scan (default: user home)");
      console.log("  -download       Download latest IOCs from Wiz Research GitHub");
      console.log("  -h, --help      Show this help message");
      process.exit(0);
    }
  }

  return flags;
}

// Helper: Get proxy URL from environment variables
function getProxyForUrl(urlStr) {
  const targetUrl = new URL(urlStr);
  const noProxy = process.env.NO_PROXY || process.env.no_proxy;

  if (noProxy) {
    if (noProxy === "*") return null;
    const noProxyList = noProxy.split(",").map(s => s.trim());
    for (const domain of noProxyList) {
      if (domain === targetUrl.hostname ||
        (domain.startsWith(".") && targetUrl.hostname.endsWith(domain)) ||
        targetUrl.hostname.endsWith(`.${domain}`)) {
        return null;
      }
    }
  }

  if (targetUrl.protocol === "https:") {
    return process.env.HTTPS_PROXY || process.env.https_proxy || process.env.HTTP_PROXY || process.env.http_proxy;
  }
  return process.env.HTTP_PROXY || process.env.http_proxy;
}

// Helper: Make HTTP/HTTPS request with proxy support
function makeRequest(urlStr) {
  return new Promise((resolve, reject) => {
    const targetUrl = new URL(urlStr);
    const proxyUrlStr = getProxyForUrl(urlStr);

    if (!proxyUrlStr) {
      const client = targetUrl.protocol === "https:" ? https : http;
      const req = client.get(urlStr, resolve);
      req.on("error", reject);
      req.setTimeout(30000, () => {
        req.destroy();
        reject(new Error("Request timeout"));
      });
      return;
    }

    const proxyUrl = new URL(proxyUrlStr);
    const proxyOpts = {
      hostname: proxyUrl.hostname,
      port: proxyUrl.port || (proxyUrl.protocol === "https:" ? 443 : 80),
      method: "CONNECT",
      path: `${targetUrl.hostname}:${targetUrl.port || 443}`,
      headers: {
        Host: `${targetUrl.hostname}:${targetUrl.port || 443}`
      }
    };

    if (proxyUrl.username) {
      const auth = `${proxyUrl.username}:${proxyUrl.password}`;
      proxyOpts.headers["Proxy-Authorization"] = `Basic ${Buffer.from(auth).toString("base64")}`;
    }

    const proxyClient = proxyUrl.protocol === "https:" ? https : http;
    const req = proxyClient.request(proxyOpts);

    req.on("connect", (res, socket, head) => {
      if (res.statusCode !== 200) {
        reject(new Error(`Proxy connection failed: ${res.statusCode} ${res.statusMessage}`));
        return;
      }

      const agent = new https.Agent({ socket });
      const realReq = https.get({
        hostname: targetUrl.hostname,
        path: targetUrl.pathname + targetUrl.search,
        agent
      }, resolve);

      realReq.on("error", reject);
    });

    req.on("error", reject);
    req.setTimeout(30000, () => {
      req.destroy();
      reject(new Error("Request timeout"));
    });
    req.end();
  });
}

// Download IOCs from Wiz Research GitHub
async function downloadWizIOCs() {
  const url = "https://raw.githubusercontent.com/wiz-sec-public/wiz-research-iocs/main/reports/shai-hulud-2-packages.csv";

  return new Promise((resolve, reject) => {
    makeRequest(url)
      .then((res) => {
        if (res.statusCode !== 200) {
          reject(new Error(`Unexpected status code: ${res.statusCode}`));
          return;
        }

        const chunks = [];
        res.on("data", (chunk) => chunks.push(chunk));
        res.on("end", () => {
          const content = Buffer.concat(chunks);
          const tmpFile = path.join(
            os.tmpdir(),
            `shai-hulud-iocs-${Date.now()}.csv`
          );

          writeFile(tmpFile, content)
            .then(() => resolve(tmpFile))
            .catch(reject);
        });
      })
      .catch(reject);
  });
}

// Parse Wiz format version strings: "= 0.0.7" or "= 0.0.7 || = 0.0.8"
function parseWizVersions(versionStr) {
  if (!versionStr || versionStr.trim() === "") {
    return [];
  }

  const parts = versionStr.split(" || ");
  const versions = [];

  for (const part of parts) {
    let v = part.trim();
    if (v.startsWith("= ")) {
      v = v.substring(2).trim();
    }
    if (v !== "") {
      versions.push(v);
    }
  }

  return versions;
}

// Read CSV and extract package versions
function readCSVPackageVersions(csvPath) {
  const content = fs.readFileSync(csvPath, "utf-8");
  const lines = content.trim().split(/\r?\n/);
  const packages = new Map();

  // Skip header line
  for (let i = 1; i < lines.length; i++) {
    const line = lines[i].trim();
    if (!line) continue;

    // Parse CSV (handling quoted fields)
    const parts = [];
    let current = "";
    let inQuotes = false;
    for (let j = 0; j < line.length; j++) {
      const char = line[j];
      if (char === `"`) {
        inQuotes = !inQuotes;
      } else if (char === "," && !inQuotes) {
        parts.push(current.trim());
        current = "";
      } else {
        current += char;
      }
    }
    parts.push(current.trim());

    if (parts.length >= 2) {
      const name = parts[0].trim();
      const versionStr = parts[1].trim();

      if (!name) continue;

      const versions = parseWizVersions(versionStr);
      if (versions.length === 0) continue;

      for (const version of versions) {
        if (version) {
          const key = `${name}@${version}`;
          packages.set(key, { name, version });
        }
      }
    }
  }

  return packages;
}

// Find all node_modules directories recursively
async function findNodeModulesDirs(root) {
  const dirs = [];
  const visited = new Set();

  async function walk(dir) {
    try {
      const entries = await readdir(dir, { withFileTypes: true });

      for (const entry of entries) {
        if (entry.isDirectory()) {
          const fullPath = path.join(dir, entry.name);

          // Skip if already visited (symlinks)
          const realPath = fs.realpathSync(fullPath);
          if (visited.has(realPath)) continue;
          visited.add(realPath);

          if (entry.name === "node_modules") {
            dirs.push(fullPath);
            // Don't recurse into node_modules
            continue;
          }

          try {
            await walk(fullPath);
          } catch (err) {
            // Skip permission denied errors
          }
        }
      }
    } catch (err) {
      // Skip permission denied and other errors
    }
  }

  try {
    await walk(root);
  } catch (err) {
    // Continue even if there are errors
  }

  return dirs;
}

// Check package for compromised version and attack indicators
function checkPackage(pkgPath, pkgName, packageVersions) {
  const indicators = [];
  let foundPV = null;

  try {
    const packageJSONPath = path.join(pkgPath, "package.json");
    const data = fs.readFileSync(packageJSONPath, "utf-8");
    const pkg = JSON.parse(data);

    // Check if this exact package@version is compromised
    const key = `${pkgName}@${pkg.version}`;
    if (packageVersions.has(key)) {
      foundPV = packageVersions.get(key);
    }

    // Check for sha1-hulud attack indicators - payload files
    const suspiciousFiles = [
      "setup_bun.js",
      "bun_environment.js",
      "actionsSecrets.json",
      "cloud.json",
      "contents.json",
      "environment.json",
      "truffleSecrets.json",
    ];

    for (const file of suspiciousFiles) {
      const filePath = path.join(pkgPath, file);
      try {
        if (fs.statSync(filePath).isFile()) {
          indicators.push(
            `Suspicious file "${file}" found in ${pkgName}@${pkg.version} at ${pkgPath}`
          );
        }
      } catch (err) {
        // File doesn't exist
      }
    }

    // Check for suspicious preinstall scripts
    const preinstall = (pkg.scripts?.preinstall || "").toLowerCase();
    const suspiciousPatterns = [
      "setup_bun",
      "bun.sh/install",
      "bun_environment",
      "cloud.json",
      "contents.json",
      "environment.json",
      "truffleSecrets",
    ];

    for (const pattern of suspiciousPatterns) {
      if (preinstall.includes(pattern)) {
        indicators.push(
          `Suspicious preinstall script pattern "${pattern}" in ${pkgName}@${pkg.version} at ${pkgPath}`
        );
        break;
      }
    }
  } catch (err) {
    // Package.json doesn't exist or is invalid
  }

  return { foundPV, indicators };
}

// Scan a single node_modules directory
function scanNodeModulesDir(dir, packageVersions) {
  const found = new Map();
  const indicators = [];

  try {
    const entries = fs.readdirSync(dir, { withFileTypes: true });

    for (const entry of entries) {
      if (!entry.isDirectory()) continue;

      const pkgName = entry.name;
      const pkgPath = path.join(dir, pkgName);

      if (pkgName.startsWith("@")) {
        // Handle scoped packages
        try {
          const scopedEntries = fs.readdirSync(pkgPath, { withFileTypes: true });
          for (const scopedEntry of scopedEntries) {
            if (!scopedEntry.isDirectory()) continue;
            const fullName = `${pkgName}/${scopedEntry.name}`;
            const scopedPath = path.join(pkgPath, scopedEntry.name);
            const { foundPV, indicators: inds } = checkPackage(
              scopedPath,
              fullName,
              packageVersions
            );
            if (foundPV) {
              const key = `${foundPV.name}@${foundPV.version}`;
              if (!found.has(key)) {
                found.set(key, []);
              }
              found.get(key).push(scopedPath);
            }
            indicators.push(...inds);
          }
        } catch (err) {
          // Skip scoped package directory if error
        }
      } else {
        const { foundPV, indicators: inds } = checkPackage(
          pkgPath,
          pkgName,
          packageVersions
        );
        if (foundPV) {
          const key = `${foundPV.name}@${foundPV.version}`;
          if (!found.has(key)) {
            found.set(key, []);
          }
          found.get(key).push(pkgPath);
        }
        indicators.push(...inds);
      }
    }
  } catch (err) {
    // Skip directory if error
  }

  return { found, indicators };
}

// ========== package.json Scanning ==========

// Check if a compromised version matches the version range
// Handles: exact versions, ^, ~, >=, <=, >, <, and combinations
function versionRangeMatches(rangeStr, compromisedVersion) {
  rangeStr = rangeStr.trim();
  if (!rangeStr) {
    return false;
  }

  // Skip non-semver ranges (git, http, file, etc.)
  if (
    rangeStr.startsWith("git+") ||
    rangeStr.startsWith("http://") ||
    rangeStr.startsWith("https://") ||
    rangeStr.startsWith("file:") ||
    rangeStr.startsWith("workspace:")
  ) {
    return false;
  }

  // Exact version match
  if (rangeStr === compromisedVersion) {
    return true;
  }

  // Handle version ranges with || (OR operator)
  if (rangeStr.includes("||")) {
    const parts = rangeStr.split("||");
    for (const part of parts) {
      if (versionRangeMatches(part.trim(), compromisedVersion)) {
        return true;
      }
    }
    return false;
  }

  // Remove prefixes like ^, ~, >=, <=, >, <
  let cleanRange = rangeStr;
  const hasCaret = cleanRange.startsWith("^");
  const hasTilde = cleanRange.startsWith("~");
  const hasGte = cleanRange.startsWith(">=");
  const hasLte = cleanRange.startsWith("<=");
  const hasGt = cleanRange.startsWith(">");
  const hasLt = cleanRange.startsWith("<");

  if (hasCaret) {
    cleanRange = cleanRange.substring(1);
  } else if (hasTilde) {
    cleanRange = cleanRange.substring(1);
  } else if (hasGte) {
    cleanRange = cleanRange.substring(2);
  } else if (hasLte) {
    cleanRange = cleanRange.substring(2);
  } else if (hasGt) {
    cleanRange = cleanRange.substring(1);
  } else if (hasLt) {
    cleanRange = cleanRange.substring(1);
  }
  cleanRange = cleanRange.trim();

  // Handle version ranges with - (range)
  if (cleanRange.includes(" - ")) {
    const parts = cleanRange.split(" - ");
    if (parts.length === 2) {
      return (
        versionRangeMatches(">=" + parts[0].trim(), compromisedVersion) &&
        versionRangeMatches("<=" + parts[1].trim(), compromisedVersion)
      );
    }
  }

  // Simple semver comparison for ^ and ~
  if (hasCaret || hasTilde) {
    const baseParts = cleanRange.split(".");
    const compParts = compromisedVersion.split(".");

    if (baseParts.length > 0 && compParts.length > 0) {
      // For ^: allow same major version
      if (hasCaret) {
        if (baseParts[0] === compParts[0]) {
          return true;
        }
      }
      // For ~: allow same major.minor version
      if (hasTilde) {
        if (baseParts.length > 1 && compParts.length > 1) {
          if (baseParts[0] === compParts[0] && baseParts[1] === compParts[1]) {
            return true;
          }
        } else if (baseParts[0] === compParts[0]) {
          return true;
        }
      }
    }
  }

  // For >=, <=, >, <, do simple string comparison
  if (hasGte || hasGt || hasLte || hasLt) {
    // Simple comparison: if versions are equal, check operator
    if (cleanRange === compromisedVersion) {
      return hasGte || hasLte;
    }
    // For approximate comparison, check if compromised version starts with cleanRange
    if (compromisedVersion.startsWith(cleanRange)) {
      return true;
    }
  }

  return false;
}

// Scan a single package.json file for compromised dependencies
function scanPackageJSONFile(packageJSONPath, packageVersions) {
  const found = new Map();
  const indicators = [];

  try {
    const data = fs.readFileSync(packageJSONPath, "utf-8");
    const pkg = JSON.parse(data);

    // Find the node_modules directory for this package.json
    const packageDir = path.dirname(packageJSONPath);
    const nodeModulesDir = path.join(packageDir, "node_modules");

    // Check all dependency types
    const dependencyTypes = [
      { name: "dependencies", deps: pkg.dependencies },
      { name: "devDependencies", deps: pkg.devDependencies },
      { name: "peerDependencies", deps: pkg.peerDependencies },
      { name: "optionalDependencies", deps: pkg.optionalDependencies },
    ];

    for (const depType of dependencyTypes) {
      if (!depType.deps) {
        continue;
      }

      for (const [depName, depVersionRange] of Object.entries(depType.deps)) {
        // Check each compromised version to see if it matches this range
        for (const [key, pv] of packageVersions.entries()) {
          if (pv.name === depName) {
            if (versionRangeMatches(depVersionRange, pv.version)) {
              // Verify the package is actually installed before reporting
              let isInstalled = false;
              let installedVersion = null;

              if (fs.existsSync(nodeModulesDir)) {
                // Check for scoped packages (@org/package)
                if (depName.startsWith("@")) {
                  const parts = depName.split("/");
                  if (parts.length === 2) {
                    const scopeDir = path.join(nodeModulesDir, parts[0]);
                    const pkgDir = path.join(scopeDir, parts[1]);
                    if (fs.existsSync(pkgDir)) {
                      try {
                        const pkgJsonPath = path.join(pkgDir, "package.json");
                        const pkgData = fs.readFileSync(pkgJsonPath, "utf-8");
                        const installedPkg = JSON.parse(pkgData);
                        installedVersion = installedPkg.version;
                        isInstalled = true;
                      } catch (e) {
                        // Package directory exists but can't read package.json
                      }
                    }
                  }
                } else {
                  // Regular package
                  const pkgDir = path.join(nodeModulesDir, depName);
                  if (fs.existsSync(pkgDir)) {
                    try {
                      const pkgJsonPath = path.join(pkgDir, "package.json");
                      const pkgData = fs.readFileSync(pkgJsonPath, "utf-8");
                      const installedPkg = JSON.parse(pkgData);
                      installedVersion = installedPkg.version;
                      isInstalled = true;
                    } catch (e) {
                      // Package directory exists but can't read package.json
                    }
                  }
                }

                // Only report if the package is actually installed AND matches the compromised version
                if (isInstalled && installedVersion === pv.version) {
                  const location = `${packageJSONPath} (${depType.name} in ${depName}@${installedVersion})`;
                  if (!found.has(key)) {
                    found.set(key, []);
                  }
                  found.get(key).push(location);
                }
              }
            }
          }
        }
      }
    }
  } catch (err) {
    // Package.json doesn't exist or is invalid
  }

  return { found, indicators };
}

// Find all package.json files, excluding node_modules
async function findPackageJSONFiles(root) {
  const files = [];
  const visited = new Set();

  async function walk(dir) {
    try {
      const entries = await readdir(dir, { withFileTypes: true });

      for (const entry of entries) {
        if (entry.isDirectory()) {
          const fullPath = path.join(dir, entry.name);

          // Skip node_modules directories
          if (entry.name === "node_modules") {
            continue;
          }

          // Skip if already visited (symlinks)
          const realPath = fs.realpathSync(fullPath);
          if (visited.has(realPath)) continue;
          visited.add(realPath);

          // Skip common directories that won't contain package.json
          if (entry.name.startsWith(".") && entry.name !== ".github") {
            continue;
          }

          try {
            await walk(fullPath);
          } catch (err) {
            // Skip permission denied errors
          }
        } else if (entry.name === "package.json") {
          const fullPath = path.join(dir, entry.name);
          files.push(fullPath);
        }
      }
    } catch (err) {
      // Skip permission denied and other errors
    }
  }

  try {
    await walk(root);
  } catch (err) {
    // Continue even if there are errors
  }

  return files;
}

// Scan package.json files in parallel
async function scanPackageJSONFiles(root, packageVersions) {
  const packageJSONFiles = await findPackageJSONFiles(root);
  if (packageJSONFiles.length === 0) {
    return { foundCompromised: new Map(), attackIndicators: [] };
  }

  const foundCompromised = new Map();
  const attackIndicators = [];

  // Use worker pool with concurrency limit
  const numWorkers = Math.min(os.cpus().length, packageJSONFiles.length);

  for (let i = 0; i < packageJSONFiles.length; i += numWorkers) {
    const batch = packageJSONFiles.slice(i, i + numWorkers);
    const promises = batch.map((file) => {
      return Promise.resolve().then(() => {
        return scanPackageJSONFile(file, packageVersions);
      });
    });

    const results = await Promise.all(promises);
    for (const { found, indicators } of results) {
      for (const [key, locs] of found.entries()) {
        if (!foundCompromised.has(key)) {
          foundCompromised.set(key, []);
        }
        foundCompromised.get(key).push(...locs);
      }
      attackIndicators.push(...indicators);
    }
  }

  return { foundCompromised, attackIndicators };
}

// Scan node_modules directories in parallel
async function scanNodeModulesParallel(nmDirs, packageVersions) {
  const foundCompromised = new Map();
  const attackIndicators = [];

  // Use worker pool with concurrency limit
  const numWorkers = Math.min(os.cpus().length, nmDirs.length);
  const workers = [];

  for (let i = 0; i < nmDirs.length; i += numWorkers) {
    const batch = nmDirs.slice(i, i + numWorkers);
    const promises = batch.map((dir) => {
      return Promise.resolve().then(() => {
        const { found, indicators } = scanNodeModulesDir(dir, packageVersions);
        return { found, indicators };
      });
    });

    const results = await Promise.all(promises);
    for (const { found, indicators } of results) {
      for (const [key, locs] of found.entries()) {
        if (!foundCompromised.has(key)) {
          foundCompromised.set(key, []);
        }
        foundCompromised.get(key).push(...locs);
      }
      attackIndicators.push(...indicators);
    }
  }

  return { foundCompromised, attackIndicators };
}

// Detect Node.js environments
function detectNodeEnvs() {
  const envs = [];
  const home = os.homedir();

  // System node
  try {
    const nodeVer = execSync("node -v", { encoding: "utf-8", stdio: ["ignore", "pipe", "ignore"] }).trim();
    const m = nodeVer.match(/v(.+)/);
    if (m) {
      envs.push({ manager: "system", version: m[1], binDir: null });
    }
  } catch (err) {
    // Node not found
  }

  // nvm
  const nvmDir = path.join(home, ".nvm", "versions", "node");
  if (fs.existsSync(nvmDir)) {
    try {
      const entries = fs.readdirSync(nvmDir, { withFileTypes: true });
      for (const entry of entries) {
        if (entry.isDirectory() && entry.name.startsWith("v")) {
          const ver = entry.name.substring(1);
          const bin = path.join(nvmDir, entry.name, "bin");
          envs.push({ manager: "nvm", version: ver, binDir: bin });
        }
      }
    } catch (err) {
      // Skip if error
    }
  }

  // fnm
  const fnmDir = path.join(home, ".fnm", "versions");
  if (fs.existsSync(fnmDir)) {
    try {
      const entries = fs.readdirSync(fnmDir, { withFileTypes: true });
      for (const entry of entries) {
        if (entry.isDirectory()) {
          const bin = path.join(fnmDir, entry.name, entry.name, "bin");
          envs.push({ manager: "fnm", version: entry.name, binDir: bin });
        }
      }
    } catch (err) {
      // Skip if error
    }
  }

  // asdf
  const asdfDir = path.join(home, ".asdf", "installs", "nodejs");
  if (fs.existsSync(asdfDir)) {
    try {
      const entries = fs.readdirSync(asdfDir, { withFileTypes: true });
      for (const entry of entries) {
        if (entry.isDirectory()) {
          const bin = path.join(asdfDir, entry.name, "bin");
          envs.push({ manager: "asdf", version: entry.name, binDir: bin });
        }
      }
    } catch (err) {
      // Skip if error
    }
  }

  // mise
  const miseBase = path.join(home, ".local", "share", "mise", "installs", "node");
  if (fs.existsSync(miseBase)) {
    try {
      const entries = fs.readdirSync(miseBase, { withFileTypes: true });
      for (const entry of entries) {
        if (entry.isDirectory()) {
          const binPath = path.join(miseBase, entry.name, "bin");
          if (fs.existsSync(binPath)) {
            envs.push({ manager: "mise", version: entry.name, binDir: binPath });
          }
        }
      }
    } catch (err) {
      // Skip if error
    }
  }

  return envs;
}

// Scan global packages for an environment
function scanGlobalPackages(env, packageVersions) {
  const found = new Map();
  const myEnv = { ...process.env };

  if (env.binDir) {
    myEnv.PATH = `${env.binDir}${path.delimiter}${myEnv.PATH}`;
  }

  // npm global
  try {
    const npmJson = execSync("npm ls -g --depth=0 --json", {
      encoding: "utf-8",
      stdio: ["ignore", "pipe", "ignore"],
      env: myEnv,
    });
    const obj = JSON.parse(npmJson);
    if (obj.dependencies) {
      for (const [pkgName, pkgInfo] of Object.entries(obj.dependencies)) {
        if (pkgInfo && typeof pkgInfo === "object" && pkgInfo.version) {
          const key = `${pkgName}@${pkgInfo.version}`;
          if (packageVersions.has(key)) {
            const pv = packageVersions.get(key);
            const location = `npm global (${env.manager}@${env.version})`;
            if (!found.has(key)) {
              found.set(key, []);
            }
            found.get(key).push(location);
          }
        }
      }
    }
  } catch (err) {
    // npm not available or error
  }

  // pnpm global
  try {
    const pnpmJson = execSync("pnpm ls -g --depth=0 --json", {
      encoding: "utf-8",
      stdio: ["ignore", "pipe", "ignore"],
      env: myEnv,
    });
    const obj = JSON.parse(pnpmJson);
    if (obj.dependencies) {
      for (const [pkgName, pkgInfo] of Object.entries(obj.dependencies)) {
        if (pkgInfo && typeof pkgInfo === "object" && pkgInfo.version) {
          const key = `${pkgName}@${pkgInfo.version}`;
          if (packageVersions.has(key)) {
            const pv = packageVersions.get(key);
            const location = `pnpm global (${env.manager}@${env.version})`;
            if (!found.has(key)) {
              found.set(key, []);
            }
            found.get(key).push(location);
          }
        }
      }
    }
  } catch (err) {
    // pnpm not available or error
  }

  return found;
}

// Scan Bun cache
function scanBunCache(packageVersions) {
  const found = new Map();
  const cacheDir =
    process.env.BUN_INSTALL_CACHE_DIR ||
    path.join(os.homedir(), ".bun", "install", "cache");

  if (!fs.existsSync(cacheDir)) {
    return found;
  }

  try {
    const entries = fs.readdirSync(cacheDir, { withFileTypes: true });
    for (const entry of entries) {
      if (!entry.isDirectory()) continue;

      // Bun cache format: pkg@version
      const nameVer = entry.name;
      const parts = nameVer.split("@");
      if (parts.length !== 2) continue;

      const pkgName = parts[0];
      const pkgVersion = parts[1];
      const key = `${pkgName}@${pkgVersion}`;

      if (packageVersions.has(key)) {
        const cachePath = path.join(cacheDir, nameVer);
        if (!found.has(key)) {
          found.set(key, []);
        }
        found.get(key).push(`Bun cache: ${cachePath}`);
      }
    }
  } catch (err) {
    // Skip if error
  }

  return found;
}

// Scan npm cache
function scanNpmCache(packageVersions) {
  const found = new Map();

  // Get npm cache directory
  let cacheDir;
  try {
    const npmCacheOutput = execSync("npm config get cache", {
      encoding: "utf-8",
      stdio: ["ignore", "pipe", "ignore"],
    });
    cacheDir = npmCacheOutput.trim();
    if (!cacheDir || cacheDir === "undefined" || cacheDir === "null") {
      // Fallback to default location
      cacheDir = path.join(os.homedir(), ".npm", "_cacache");
    }
  } catch (err) {
    // Fallback to default location
    cacheDir = path.join(os.homedir(), ".npm", "_cacache");
  }

  if (!fs.existsSync(cacheDir)) {
    return found;
  }

  // npm cache uses content-addressable storage, so we need to check metadata
  // Try to use npm cache ls if available, otherwise scan index files
  try {
    // Try using npm cache ls command (available in npm 5+)
    const cacheList = execSync("npm cache ls --json 2>/dev/null || npm cache ls 2>/dev/null", {
      encoding: "utf-8",
      stdio: ["ignore", "pipe", "ignore"],
      shell: true,
    });

    // Parse npm cache ls output
    // Format can be: "name@version" or JSON array
    const lines = cacheList.trim().split(/\r?\n/);
    for (const line of lines) {
      if (!line || line.trim() === "") continue;

      // Try to parse as JSON first
      let pkgInfo;
      try {
        pkgInfo = JSON.parse(line);
        if (pkgInfo.name && pkgInfo.version) {
          const key = `${pkgInfo.name}@${pkgInfo.version}`;
          if (packageVersions.has(key)) {
            if (!found.has(key)) {
              found.set(key, []);
            }
            found.get(key).push(`npm cache: ${cacheDir}`);
          }
        }
      } catch (e) {
        // Not JSON, try parsing as "name@version" format
        const match = line.match(/^(.+?)@(.+?)$/);
        if (match) {
          const [, pkgName, pkgVersion] = match;
          const key = `${pkgName}@${pkgVersion}`;
          if (packageVersions.has(key)) {
            if (!found.has(key)) {
              found.set(key, []);
            }
            found.get(key).push(`npm cache: ${cacheDir}`);
          }
        }
      }
    }
  } catch (err) {
    // npm cache ls not available or failed, try scanning index files
    try {
      const indexDir = path.join(cacheDir, "index-v5");
      if (fs.existsSync(indexDir)) {
        // npm cache index files are content-addressable, so we scan metadata
        // This is a simplified check - npm cache structure is complex
        const entries = fs.readdirSync(indexDir, { withFileTypes: true });
        for (const entry of entries) {
          if (!entry.isDirectory()) continue;

          const subDir = path.join(indexDir, entry.name);
          try {
            const subEntries = fs.readdirSync(subDir, { withFileTypes: true });
            for (const subEntry of subEntries) {
              if (subEntry.isFile() && subEntry.name.endsWith(".json")) {
                try {
                  const indexPath = path.join(subDir, subEntry.name);
                  const indexData = fs.readFileSync(indexPath, "utf-8");
                  const index = JSON.parse(indexData);

                  // Check metadata for package name/version
                  if (index.metadata && index.metadata.name && index.metadata.version) {
                    const key = `${index.metadata.name}@${index.metadata.version}`;
                    if (packageVersions.has(key)) {
                      if (!found.has(key)) {
                        found.set(key, []);
                      }
                      found.get(key).push(`npm cache: ${cacheDir}`);
                    }
                  }
                } catch (e) {
                  // Skip invalid JSON files
                }
              }
            }
          } catch (e) {
            // Skip if can't read subdirectory
          }
        }
      }
    } catch (err) {
      // Skip if can't scan cache
    }
  }

  return found;
}

// Scan global packages and cache
function scanGlobalPackagesAndCache(packageVersions) {
  const found = new Map();
  const envs = detectNodeEnvs();

  if (envs.length > 0) {
    console.log(`  Detected ${envs.length} Node.js environment(s)`);
  }

  for (const env of envs) {
    const globalFound = scanGlobalPackages(env, packageVersions);
    for (const [key, locs] of globalFound.entries()) {
      if (!found.has(key)) {
        found.set(key, []);
      }
      found.get(key).push(...locs);
    }
  }

  const bunFound = scanBunCache(packageVersions);
  for (const [key, locs] of bunFound.entries()) {
    if (!found.has(key)) {
      found.set(key, []);
    }
    found.get(key).push(...locs);
  }

  const npmFound = scanNpmCache(packageVersions);
  for (const [key, locs] of npmFound.entries()) {
    if (!found.has(key)) {
      found.set(key, []);
    }
    found.get(key).push(...locs);
  }

  return found;
}

// Scan GitHub workflows for malicious patterns
async function scanGitHubWorkflows(root) {
  const indicators = [];
  const workflowDirs = [];

  async function findWorkflowDirs(dir) {
    try {
      const entries = await readdir(dir, { withFileTypes: true });

      for (const entry of entries) {
        if (entry.isDirectory()) {
          const fullPath = path.join(dir, entry.name);

          if (entry.name === ".github") {
            const workflowsPath = path.join(fullPath, "workflows");
            try {
              const stat = await fs.promises.stat(workflowsPath);
              if (stat.isDirectory()) {
                workflowDirs.push(workflowsPath);
              }
            } catch (err) {
              // workflows directory doesn't exist
            }
            // Don't recurse into .github
            continue;
          }

          // Skip common directories
          if (entry.name.startsWith(".") && entry.name !== ".github") {
            continue;
          }

          try {
            await findWorkflowDirs(fullPath);
          } catch (err) {
            // Skip permission denied errors
          }
        }
      }
    } catch (err) {
      // Skip permission denied and other errors
    }
  }

  try {
    await findWorkflowDirs(root);
  } catch (err) {
    // Continue even if there are errors
  }

  for (const workflowDir of workflowDirs) {
    try {
      const entries = await readdir(workflowDir, { withFileTypes: true });

      for (const entry of entries) {
        if (entry.isDirectory()) continue;

        const fileName = entry.name;
        const filePath = path.join(workflowDir, fileName);

        // Check for suspicious workflow files
        if (fileName === "discussion.yaml" || fileName === "discussion.yml") {
          try {
            const content = await readFile(filePath, "utf-8");
            if (
              content.includes("self-hosted") &&
              (content.includes("github.event.discussion.body") ||
                content.includes("RUNNER_TRACKING_ID"))
            ) {
              indicators.push(
                `Suspicious GitHub workflow file: ${filePath} (potential backdoor)`
              );
            }
          } catch (err) {
            // Skip if can't read
          }
        }

        // Check for formatter workflow files
        if (
          fileName.startsWith("formatter_") &&
          (fileName.endsWith(".yml") || fileName.endsWith(".yaml"))
        ) {
          try {
            const content = await readFile(filePath, "utf-8");
            if (
              content.includes("toJSON(secrets)") ||
              content.includes("actionsSecrets")
            ) {
              indicators.push(
                `Suspicious GitHub workflow file: ${filePath} (potential secret exfiltration)`
              );
            }
          } catch (err) {
            // Skip if can't read
          }
        }
      }
    } catch (err) {
      // Skip if can't read directory
    }
  }

  return indicators;
}

// Main function
async function main() {
  const flags = parseArgs();

  let finalCSVPath;
  let err;

  if (flags.download) {
    console.log("ðŸ“¥ Downloading latest Shai-Hulud IOCs from Wiz Research...");
    try {
      finalCSVPath = await downloadWizIOCs();
      console.log(`âœ… Downloaded IOCs to: ${finalCSVPath}`);
    } catch (downloadErr) {
      console.error(`Error downloading IOCs: ${downloadErr.message}`);
      console.log("Falling back to local CSV if provided...");
      if (!flags.csv) {
        console.error(
          "Please specify -csv /path/to/file.csv or ensure -download can access the internet"
        );
        process.exit(1);
      }
      finalCSVPath = flags.csv;
    }
  } else {
    if (!flags.csv) {
      console.error(
        "Please specify -csv /path/to/file.csv or use -download to fetch latest"
      );
      console.error(
        "CSV format: Package,Version (supports Wiz format with '= ' prefix and '||' separators)"
      );
      process.exit(1);
    }
    finalCSVPath = flags.csv;
  }

  let packageVersions;
  try {
    packageVersions = readCSVPackageVersions(finalCSVPath);
  } catch (readErr) {
    console.error(`Error reading CSV: ${readErr.message}`);
    process.exit(1);
  }

  console.log(`ðŸ“‹ Loaded ${packageVersions.size} compromised package versions`);

  const scanRoot = flags.root || os.homedir() || "/";
  console.log("ðŸ” Scanning for sha1-hulud malicious packages...");
  console.log(`Scanning under: ${scanRoot}`);

  const nmDirs = await findNodeModulesDirs(scanRoot);
  console.log(`Found ${nmDirs.length} node_modules directories`);

  if (nmDirs.length > 0) {
    const numWorkers = Math.min(os.cpus().length, nmDirs.length);
    console.log(`Using ${numWorkers} parallel workers`);
  }

  // Parallel scanning
  const { foundCompromised, attackIndicators } =
    await scanNodeModulesParallel(nmDirs, packageVersions);

  // Scan global packages and caches (npm, Bun)
  console.log("\nðŸ” Scanning global packages and package manager caches...");
  const globalCompromised = scanGlobalPackagesAndCache(packageVersions);
  for (const [key, locs] of globalCompromised.entries()) {
    if (!foundCompromised.has(key)) {
      foundCompromised.set(key, []);
    }
    foundCompromised.get(key).push(...locs);
  }

  // Scan package.json files for compromised dependencies
  console.log("\nðŸ” Scanning package.json files for compromised dependencies...");
  const { foundCompromised: packageJSONCompromised, attackIndicators: packageJSONIndicators } =
    await scanPackageJSONFiles(scanRoot, packageVersions);
  if (packageJSONCompromised.size > 0) {
    console.log(`Found compromised dependencies in ${packageJSONCompromised.size} package.json file(s)`);
  }
  for (const [key, locs] of packageJSONCompromised.entries()) {
    if (!foundCompromised.has(key)) {
      foundCompromised.set(key, []);
    }
    foundCompromised.get(key).push(...locs);
  }
  attackIndicators.push(...packageJSONIndicators);

  // Scan for GitHub workflow files
  console.log("\nðŸ” Scanning for malicious GitHub workflows...");
  const workflowIndicators = await scanGitHubWorkflows(scanRoot);
  if (workflowIndicators.length > 0) {
    console.log(`Found ${workflowIndicators.length} suspicious workflow files`);
  }
  const allAttackIndicators = [...attackIndicators, ...workflowIndicators];

  // Report
  console.log("\n" + "=".repeat(60));
  console.log("SHA1-HULUD MALWARE SCAN REPORT");
  console.log("=".repeat(60));

  let hasFindings = false;

  if (foundCompromised.size > 0) {
    hasFindings = true;
    console.log("\nðŸ”´ COMPROMISED PACKAGES DETECTED:");
    for (const [key, locs] of foundCompromised.entries()) {
      const [name, version] = key.split("@");
      console.log(`\n  Package: ${name}@${version}`);
      for (const loc of locs) {
        console.log(`    â””â”€ ${loc}`);
      }
    }
  }

  if (allAttackIndicators.length > 0) {
    hasFindings = true;
    console.log("\nâš ï¸  ATTACK INDICATORS FOUND:");
    for (const indicator of allAttackIndicators) {
      console.log(`  â€¢ ${indicator}`);
    }
  }

  if (hasFindings) {
    console.log("\n" + "=".repeat(60));
    console.log("ðŸš¨ IMMEDIATE ACTIONS REQUIRED:");
    console.log("=".repeat(60));
    console.log("1. Rotate ALL credentials immediately:");
    console.log("   - GitHub tokens & SSH keys");
    console.log("   - npm tokens");
    console.log("   - AWS, GCP, Azure credentials");
    console.log("\n2. Check GitHub for malicious repos:");
    console.log("   - Search for repos with 'Sha1-Hulud: The Second Coming'");
    console.log("   - Delete any unauthorized repos");
    console.log("\n3. Review GitHub Actions:");
    console.log("   - Check .github/workflows/ for suspicious files");
    console.log("   - Remove any unauthorized self-hosted runners");
    console.log("\n4. Remove compromised packages:");
    console.log("   - Delete and reinstall from clean versions");
    console.log("   - Update to versions published before Nov 21, 2025");
    console.log("\n5. Clear package manager caches:");
    console.log("   - npm:  npm cache clean --force");
    console.log("   - pnpm: pnpm store prune");
    console.log("   - yarn: yarn cache clean");
    console.log("   - Bun:  rm -rf ~/.bun/install/cache");
    console.log("   This removes potentially compromised packages from cache");
    console.log("=".repeat(60));
    process.exit(1);
  } else {
    console.log("\nâœ… No sha1-hulud compromised packages detected");
    console.log(
      "\nðŸ’¡ Stay vigilant: New compromised packages may still be discovered."
    );
  }
}

// Run main
main().catch((err) => {
  console.error("Fatal error:", err);
  process.exit(1);
});
